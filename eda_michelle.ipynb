{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blThwcZo0Q-V"
   },
   "source": [
    "### Research questions\n",
    "1. prediction problem (classification) and feature weights - simply predict the next sentiment based on generated features, and analyse which feature contributes the most. examples: user-id (same thing) date (time), text (key words).\n",
    "2. incoperate with LLM to give explanations of why the text is classified as given sentiment.\n",
    "3. efficient forecasting over large datasets, create a basic model, and compared two ways of processing data. 1, deploy locally and use naive python packages. 2, utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1744828695063,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "gCxBRGXvwynN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GroupKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "error",
     "timestamp": 1744828701560,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "4XMFiO5Lw3RQ",
    "outputId": "0fd86710-1545-44ab-fb06-88c2fcbf0ed6"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path, encoding='latin-1', header=None)\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "    \n",
    "    # Convert sentiment to binary (0: negative, 1: positive)\n",
    "    # Assuming sentiment values are 0 and 4 in the original dataset\n",
    "    df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S PDT %Y')\n",
    "    \n",
    "    # Extract basic features from text\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['hashtag_count'] = df['text'].str.count(r'#')\n",
    "    df['mention_count'] = df['text'].str.count(r'@')\n",
    "    df['url_count'] = df['text'].str.count(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text data by removing @mentions, URLs, hashtags, punctuation\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs - more comprehensive pattern\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions - more comprehensive pattern\n",
    "    text = re.sub(r'@[\\w_]+', '', text)\n",
    "    \n",
    "    # Remove hashtags (but keep the text after #)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove digits (optional - uncomment if needed)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace (including newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Function to debug text cleaning process\n",
    "    \"\"\"\n",
    "    print(\"Original:\", text)\n",
    "    \n",
    "    # Test URL removal\n",
    "    text_no_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    print(\"After URL removal:\", text_no_urls)\n",
    "    \n",
    "    # Test @mention removal\n",
    "    text_no_mentions = re.sub(r'@[\\w_]+', '', text_no_urls)\n",
    "    print(\"After @mention removal:\", text_no_mentions)\n",
    "    \n",
    "    # Test hashtag conversion\n",
    "    text_no_hashtags = re.sub(r'#(\\w+)', r'\\1', text_no_mentions)\n",
    "    print(\"After hashtag conversion:\", text_no_hashtags)\n",
    "    \n",
    "    # Test punctuation removal\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', '', text_no_hashtags)\n",
    "    print(\"After punctuation removal:\", text_no_punct)\n",
    "    \n",
    "    # Test whitespace cleaning\n",
    "    text_clean = re.sub(r'\\s+', ' ', text_no_punct).strip()\n",
    "    print(\"Final cleaned:\", text_clean)\n",
    "    \n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h0QoPhx7zY4r"
   },
   "outputs": [],
   "source": [
    "def create_visualizations(df):\n",
    "    # 1. Sentiment Distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='sentiment', data=df)\n",
    "    plt.title('Distribution of Sentiments')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Text Length Distribution by Sentiment\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='sentiment', y='text_length', data=df)\n",
    "    plt.title('Text Length Distribution by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Text Length')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/text_length_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Time-based Analysis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Hourly distribution\n",
    "    sns.countplot(x='hour', hue='sentiment', data=df, ax=axes[0])\n",
    "    axes[0].set_title('Tweets by Hour of Day')\n",
    "    axes[0].set_xlabel('Hour')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Day of week distribution\n",
    "    sns.countplot(x='day_of_week', hue='sentiment', data=df, ax=axes[1])\n",
    "    axes[1].set_title('Tweets by Day of Week')\n",
    "    axes[1].set_xlabel('Day of Week (0=Monday)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Monthly distribution\n",
    "    sns.countplot(x='month', hue='sentiment', data=df, ax=axes[2])\n",
    "    axes[2].set_title('Tweets by Month')\n",
    "    axes[2].set_xlabel('Month')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/time_based_analysis.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Feature Correlation Analysis\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[['sentiment', 'text_length', 'word_count', 'hashtag_count', 'mention_count', 'url_count']].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.savefig('fig/feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Word Clouds for Positive and Negative Tweets\n",
    "    def generate_wordcloud(text, title, filename):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    \n",
    "    # Generate word clouds for positive and negative tweets\n",
    "    positive_text = ' '.join(df[df['sentiment'] == 1]['text'])\n",
    "    negative_text = ' '.join(df[df['sentiment'] == 0]['text'])\n",
    "    \n",
    "    generate_wordcloud(positive_text, 'Word Cloud for Positive Tweets', 'positive_wordcloud.png')\n",
    "    generate_wordcloud(negative_text, 'Word Cloud for Negative Tweets', 'negative_wordcloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features based on the project outline\n",
    "    \"\"\"\n",
    "    # 1. User-based Features\n",
    "    \n",
    "    # Group by user and calculate statistics\n",
    "    user_stats = df.groupby('user')['sentiment'].agg(['mean', 'count', 'std']).reset_index()\n",
    "    \n",
    "    # Calculate correct std with n-1 denominator\n",
    "    def adjusted_std(group):\n",
    "        if len(group) <= 1:\n",
    "            return 0\n",
    "        return np.std(group, ddof=1)  # ddof=1 uses n-1 denominator\n",
    "    \n",
    "    user_sentiment_std = df.groupby('user')['sentiment'].apply(adjusted_std)\n",
    "    user_stats['std'] = user_stats['user'].map(user_sentiment_std)\n",
    "    \n",
    "    # Handle case where a user has only one tweet (std is NaN)\n",
    "    user_stats['std'] = user_stats['std'].fillna(0)\n",
    "    \n",
    "    user_stats.columns = ['user', 'user_avg_sentiment', 'user_tweet_count', 'user_sentiment_std']\n",
    "    \n",
    "    # Merge user stats back to main dataframe\n",
    "    df = pd.merge(df, user_stats, on='user', how='left')\n",
    "    \n",
    "    # Calculate average posting gap time for each user\n",
    "    df = df.sort_values(['user', 'date'])\n",
    "    \n",
    "    # Function to calculate average time between posts\n",
    "    def calc_avg_gap(group):\n",
    "        if len(group) <= 1:\n",
    "            return pd.Timedelta(0)\n",
    "        gaps = group['date'].diff().dropna()\n",
    "        return gaps.mean()\n",
    "    \n",
    "    # Calculate average gap for each user\n",
    "    avg_gaps = df.groupby('user').apply(calc_avg_gap)\n",
    "    avg_gaps_seconds = avg_gaps.dt.total_seconds()\n",
    "    avg_gaps_df = pd.DataFrame({\n",
    "        'user': avg_gaps.index, \n",
    "        'avg_posting_gap_seconds': avg_gaps_seconds.values\n",
    "    })\n",
    "    \n",
    "    # Merge gaps back to main dataframe\n",
    "    df = pd.merge(df, avg_gaps_df, on='user', how='left')\n",
    "    df['avg_posting_gap_seconds'] = df['avg_posting_gap_seconds'].fillna(0)\n",
    "    \n",
    "    # 2. Text Processing Features\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Create tokenized text for Word2Vec\n",
    "    df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # These will be one-hot encoded later\n",
    "    df['hour_cat'] = df['hour'].astype('category')\n",
    "    df['day_of_week_cat'] = df['day_of_week'].astype('category') \n",
    "    df['month_cat'] = df['month'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_features(df, w2v_df):\n",
    "    \"\"\"\n",
    "    Prepare features with proper preprocessing\n",
    "    \"\"\"\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = ['hour_cat', 'day_of_week_cat', 'month_cat']\n",
    "    numerical_features = ['text_length', 'word_count', 'hashtag_count', \n",
    "                         'mention_count', 'url_count', 'user_tweet_count', \n",
    "                         'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "    \n",
    "    # Create column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Extract features from DataFrame\n",
    "    X = df[numerical_features + categorical_features]\n",
    "    \n",
    "    # Preprocess the features\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Combine with Word2Vec features (which may need their own scaling)\n",
    "    w2v_scaled = StandardScaler().fit_transform(w2v_df)\n",
    "    \n",
    "    # Convert to sparse matrices if needed for efficiency\n",
    "    from scipy import sparse\n",
    "    if sparse.issparse(X_preprocessed):\n",
    "        X_combined = sparse.hstack([X_preprocessed, w2v_scaled])\n",
    "    else:\n",
    "        X_combined = np.hstack([X_preprocessed, w2v_scaled])\n",
    "    \n",
    "    return X_combined, preprocessor\n",
    "\n",
    "\n",
    "def extract_word2vec_features(df, vector_size=50, min_count=2):\n",
    "    \"\"\"\n",
    "    Extract Word2Vec features\n",
    "    \"\"\"\n",
    "    # Train Word2Vec model\n",
    "    all_tokens = df['tokens'].tolist()\n",
    "\n",
    "      # Train with more context window and more training iterations\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=all_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=8,          # Larger context window\n",
    "        min_count=min_count, # Ignore rare words\n",
    "        workers=4,\n",
    "        sg=1,              # Use skip-gram\n",
    "        epochs=20          # More training iterations\n",
    "    )\n",
    "    \n",
    "    # Function to get document vectors by averaging word vectors\n",
    "    def get_doc_vector(tokens):\n",
    "        vec = np.zeros(vector_size)\n",
    "        count = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                vec += w2v_model.wv[word]\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                # Word not in vocabulary\n",
    "                continue\n",
    "        if count > 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    \n",
    "    # Get document vectors\n",
    "    \n",
    "    doc_vectors = np.array(df['tokens'].apply(get_doc_vector).tolist())\n",
    "    w2v_df = pd.DataFrame(\n",
    "        doc_vectors,\n",
    "        columns=[f'w2v_{i}' for i in range(vector_size)]\n",
    "    )\n",
    "    # doc_vectors = df['tokens'].apply(get_doc_vector)\n",
    "    # w2v_df = pd.DataFrame(doc_vectors, columns=['word2vec_embedding'])\n",
    "\n",
    "    return w2v_df, w2v_model\n",
    "\n",
    "def select_features(features_df, n_components=20):\n",
    "    \"\"\"\n",
    "    Perform PCA for feature selection\n",
    "    \"\"\"\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    pca_features = pca.fit_transform(features_df)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_features, \n",
    "        columns=[f'pca_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Print variance explanation\n",
    "    print(f\"Top 10 components explain {cumulative_variance[9]:.2%} of variance\")\n",
    "    print(f\"All {n_components} components explain {cumulative_variance[-1]:.2%} of variance\")\n",
    "    \n",
    "    return pca_df, pca\n",
    "\n",
    "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate classification models\n",
    "    \"\"\"\n",
    "    # Define base models\n",
    "    # Best SVM parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "    # Best Random Forest parameters: {'max_depth': 20, 'n_estimators': 200}\n",
    "    # Best XGBoost parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
    "\n",
    "    svm = SVC(probability=False, kernel='rbf', random_state=42, C=10, gamma='auto')\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=20)\n",
    "    xgb = GradientBoostingClassifier(n_estimators=200, random_state=42, learning_rate=0.1, max_depth=3) \n",
    "    \n",
    "    # Train individual models\n",
    "    models = {\n",
    "        'SVM': svm,\n",
    "        'Random Forest': rf,\n",
    "        'XGBoost': xgb\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report\n",
    "        }\n",
    "        \n",
    "        print(f\"Training {name} took {time.time() - t0:.2f} seconds\")\n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "        print(f\"Classification Report:\\n{class_report}\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # Create Voting Ensemble (majority voting)\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "        voting='hard'  # Majority voting\n",
    "    )\n",
    "    \n",
    "    print(\"Training Ensemble (Majority Voting)...\")\n",
    "    t0 = time.time()\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results['Ensemble'] = {\n",
    "        'model': voting_clf,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "    print(\"Training Ensemble took %0.2f seconds\" % (time.time() - t0))\n",
    "    print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_cross_validation(X, y, df):\n",
    "    \"\"\"\n",
    "    Perform time-based and user-based cross-validation\n",
    "    \"\"\"\n",
    "    # Time-based Cross Validation\n",
    "    print(\"Performing Time-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define base models, use voting ensemble for cross validation\n",
    "    svm = SVC(probability=False, kernel='rbf', random_state=42, C=10, gamma='auto')\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=20)\n",
    "    xgb = GradientBoostingClassifier(n_estimators=200, random_state=42, learning_rate=0.1, max_depth=3) \n",
    "    # Create Voting Ensemble (majority voting)\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "        voting='hard'  # Majority voting\n",
    "    )\n",
    "    \n",
    "    time_scores = []\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        time_scores.append(score)\n",
    "    \n",
    "    print(f\"Time-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"Time-based CV Scores: {time_scores}\")\n",
    "    print(f\"Mean Time-based CV Score: {np.mean(time_scores):.4f}\")\n",
    "    \n",
    "    # User-based Cross Validation\n",
    "    print(\"\\nPerforming User-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    user_groups = df['user'].astype('category').cat.codes.values\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    user_scores = []\n",
    "    for train_index, test_index in gkf.split(X, y, groups=user_groups):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        user_scores.append(score)\n",
    "    \n",
    "    print(f\"User-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"User-based CV Scores: {user_scores}\")\n",
    "    print(f\"Mean User-based CV Score: {np.mean(user_scores):.4f}\")\n",
    "    \n",
    "    return time_scores, user_scores\n",
    "\n",
    "def analyze_misclassified_examples(df, X_test, y_test, model, idx_test):\n",
    "    \"\"\"\n",
    "    Analyze misclassified examples\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    misclassified_idx = idx_test[y_pred != y_test]\n",
    "    \n",
    "    misclassified_df = df.iloc[misclassified_idx].copy()\n",
    "    misclassified_df['predicted_sentiment'] = y_pred[y_pred != y_test]\n",
    "    \n",
    "    print(f\"Number of misclassified examples: {len(misclassified_df)}\")\n",
    "    \n",
    "    # Analyze by features\n",
    "    print(\"\\nMisclassification Analysis by Features:\")\n",
    "    \n",
    "    # By text length\n",
    "    print(\"\\nBy Text Length:\")\n",
    "    bins = [0, 50, 100, 150, 200, np.inf]\n",
    "    labels = ['Very Short', 'Short', 'Medium', 'Long', 'Very Long']\n",
    "    misclassified_df['text_length_bin'] = pd.cut(misclassified_df['text_length'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['text_length_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By user tweet count\n",
    "    print(\"\\nBy User Tweet Count:\")\n",
    "    bins = [0, 5, 10, 20, 50, np.inf]\n",
    "    labels = ['Very Few', 'Few', 'Average', 'Many', 'Very Many']\n",
    "    misclassified_df['user_tweet_count_bin'] = pd.cut(misclassified_df['user_tweet_count'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['user_tweet_count_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By time of day\n",
    "    print(\"\\nBy Hour of Day:\")\n",
    "    hour_bins = [0, 6, 12, 18, 24]\n",
    "    hour_labels = ['Night', 'Morning', 'Afternoon', 'Evening']\n",
    "    misclassified_df['hour_bin'] = pd.cut(misclassified_df['hour'], bins=hour_bins, labels=hour_labels)\n",
    "    print(misclassified_df['hour_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Sample of misclassified examples\n",
    "    print(\"\\nSample of Misclassified Examples:\")\n",
    "    sample = misclassified_df.sample(min(5, len(misclassified_df)))\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"Text: {row['text']}\")\n",
    "        print(f\"True Sentiment: {row['sentiment']}, Predicted: {row['predicted_sentiment']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return misclassified_df\n",
    "\n",
    "def visualize_results(df, results, pca, feature_names):\n",
    "    \"\"\"\n",
    "    Create visualizations for the analysis\n",
    "    \"\"\"\n",
    "    # 1. PCA Explained Variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Principal Component')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/pca_variance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Feature Importance from PCA loadings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Get most important features from first component\n",
    "    component = 0\n",
    "    loadings = pd.Series(abs(pca.components_[component]), index=feature_names)\n",
    "    top_features = loadings.nlargest(15)\n",
    "    \n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.title(f'Top 15 Feature Importances (PC {component+1})')\n",
    "    plt.xlabel('Absolute Loading Value')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Sentiment Distribution by Time of Day\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hour_counts = df.groupby(['hour', 'sentiment']).size().unstack()\n",
    "    hour_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_hour.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Sentiment Distribution by Day of Week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_counts = df.groupby(['day_of_week', 'sentiment']).size().unstack()\n",
    "    day_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Day of Week')\n",
    "    plt.xlabel('Day of Week (0=Monday)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_day.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. User Sentiment Patterns (Top 10 users by tweet count)\n",
    "    top_users = df['user'].value_counts().head(10).index\n",
    "    user_df = df[df['user'].isin(top_users)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    user_sentiment = user_df.groupby('user')['sentiment'].mean().sort_values()\n",
    "    sns.barplot(x=user_sentiment.index, y=user_sentiment.values)\n",
    "    plt.title('Average Sentiment for Top 10 Users')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/user_sentiment.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Word Clouds by Sentiment\n",
    "    for sentiment, label in [(0, 'Negative'), (1, 'Positive')]:\n",
    "        text = ' '.join(df[df['sentiment'] == sentiment]['clean_text'])\n",
    "        \n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            max_words=200\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for {label} Sentiment')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/wordcloud_sentiment_{sentiment}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Model Comparison\n",
    "    accuracies = {name: info['accuracy'] for name, info in results.items()}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Confusion Matrix Visualization\n",
    "    for name, info in results.items():\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = info['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/confusion_matrix_{name}.png')\n",
    "        plt.close()\n",
    "\n",
    "def compare_processing_methods(df, test_size=1000):\n",
    "    \"\"\"\n",
    "    Compare local vs distributed processing performance\n",
    "    \"\"\"\n",
    "    # Subset data for testing\n",
    "    test_df = df.sample(test_size, random_state=42)\n",
    "    \n",
    "    # 1. Local Python Implementation\n",
    "    print(\"Testing Local Python Implementation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate local processing\n",
    "    tokens = test_df['text'].apply(clean_text).apply(word_tokenize).tolist()\n",
    "    local_time = time.time() - start_time\n",
    "    print(f\"Local processing time: {local_time:.2f} seconds\")\n",
    "    \n",
    "    # 2. Simulated Distributed Processing\n",
    "    try:\n",
    "        print(\"\\nTesting Parallel Processing Implementation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine number of cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(f\"Using {num_cores} cores\")\n",
    "        \n",
    "        # Split data into chunks\n",
    "        chunks = np.array_split(test_df['text'], num_cores)\n",
    "        \n",
    "        # Define processing function\n",
    "        def process_chunk(chunk):\n",
    "            return [word_tokenize(clean_text(text)) for text in chunk]\n",
    "        \n",
    "        # Create a pool and process in parallel\n",
    "        with multiprocessing.Pool(num_cores) as pool:\n",
    "            results = pool.map(process_chunk, chunks)\n",
    "            \n",
    "        # Flatten results\n",
    "        parallel_tokens = [item for sublist in results for item in sublist]\n",
    "        \n",
    "        parallel_time = time.time() - start_time\n",
    "        print(f\"Parallel processing time: {parallel_time:.2f} seconds\")\n",
    "        print(f\"Speedup: {local_time / parallel_time:.2f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in parallel processing: {e}\")\n",
    "        print(\"Please set up a proper distributed environment for actual testing\")\n",
    "        parallel_time = None\n",
    "    \n",
    "    return {'local_time': local_time, 'parallel_time': parallel_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_with_selective_pca(df, w2v_df, num_pca_components=10):\n",
    "    \"\"\"\n",
    "    Apply PCA selectively to numerical features only, then combine with\n",
    "    categorical features and Word2Vec embeddings\n",
    "    \"\"\"\n",
    "    # Define feature groups\n",
    "    numerical_features = ['word_count', 'hashtag_count', \n",
    "                         'mention_count', 'url_count', 'user_tweet_count', \n",
    "                         'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "    \n",
    "    categorical_features = ['hour', 'day_of_week', 'month']\n",
    "    \n",
    "    # 1. Extract and standardize numerical features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    numerical_scaled = scaler.fit_transform(df[numerical_features])\n",
    "    \n",
    "    # 2. Apply PCA to numerical features only\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=min(num_pca_components, len(numerical_features)))\n",
    "    numerical_pca = pca.fit_transform(numerical_scaled)\n",
    "    \n",
    "    # Print variance explained by numerical PCA\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    print(f\"Numerical PCA: {pca.n_components_} components explain {cumulative_variance[-1]:.2%} of variance\")\n",
    "    \n",
    "    # 3. One-hot encode categorical features\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
    "    \n",
    "    # 4. Combine all features: numerical_pca + categorical_encoded + w2v_df\n",
    "    \n",
    "    # Convert to numeric arrays if needed\n",
    "    numerical_pca_array = numerical_pca\n",
    "    w2v_array = w2v_df.values\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_features = np.hstack([\n",
    "        numerical_pca_array,   # PCA-reduced numerical features\n",
    "        categorical_encoded,   # One-hot encoded categorical features\n",
    "        w2v_array              # Word2Vec embeddings\n",
    "    ])\n",
    "    \n",
    "    # Create feature names for interpretability\n",
    "    feature_names = (\n",
    "        [f'num_pca_{i}' for i in range(pca.n_components_)] +\n",
    "        [f'{feat}_{cat}' for feat, cats in zip(encoder.feature_names_in_, \n",
    "                                              encoder.categories_) \n",
    "                          for cat in cats[1:]] +\n",
    "        list(w2v_df.columns)\n",
    "    )\n",
    "    \n",
    "    # Return as DataFrame for convenience\n",
    "    combined_df = pd.DataFrame(combined_features, columns=feature_names)\n",
    "    \n",
    "    return combined_df, pca, encoder\n",
    "\n",
    "\n",
    "def analyze_pca_components(pca, feature_names, n_components=4, n_top_features=4):\n",
    "    \"\"\"\n",
    "    Analyze PCA components and print the top features for each component\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca : PCA\n",
    "        Fitted PCA model\n",
    "    feature_names : list\n",
    "        Names of the features used in PCA\n",
    "    n_components : int\n",
    "        Number of principal components to analyze\n",
    "    n_top_features : int\n",
    "        Number of top features to display for each component\n",
    "    \"\"\"\n",
    "    # Check if we have fewer components than requested\n",
    "    n_components = min(n_components, pca.n_components_)\n",
    "    \n",
    "    print(f\"\\nTop {n_top_features} features for each of the first {n_components} principal components:\")\n",
    "    \n",
    "    # For each component\n",
    "    for i in range(n_components):\n",
    "        # Get loadings (weights) for this component\n",
    "        loadings = pca.components_[i]\n",
    "        \n",
    "        # Get indices of top features (highest absolute loadings)\n",
    "        top_indices = np.argsort(np.abs(loadings))[-n_top_features:]\n",
    "        \n",
    "        # Reverse to get highest first\n",
    "        top_indices = top_indices[::-1]\n",
    "        \n",
    "        # Print component number and variance explained\n",
    "        variance = pca.explained_variance_ratio_[i]\n",
    "        print(f\"\\nComponent {i+1} (explains {variance:.2%} of variance):\")\n",
    "        \n",
    "        # Print top features\n",
    "        for idx in top_indices:\n",
    "            # Get feature name and loading\n",
    "            feature = feature_names[idx]\n",
    "            loading = loadings[idx]\n",
    "            sign = \"+\" if loading > 0 else \"-\"\n",
    "            \n",
    "            # Print feature and its loading\n",
    "            print(f\"  {sign} {feature}: {abs(loading):.4f}\")\n",
    "    \n",
    "    # Print cumulative variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nCumulative variance explained by these {n_components} components: {cumulative_variance[n_components-1]:.2%}\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                date     query             user  \\\n",
       "0          0  1467810369 2009-04-06 22:19:45  NO_QUERY  _TheSpecialOne_   \n",
       "1          0  1467810672 2009-04-06 22:19:49  NO_QUERY    scotthamilton   \n",
       "2          0  1467810917 2009-04-06 22:19:53  NO_QUERY         mattycus   \n",
       "3          0  1467811184 2009-04-06 22:19:57  NO_QUERY          ElleCTF   \n",
       "4          0  1467811193 2009-04-06 22:19:57  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  text_length  word_count  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          115          19   \n",
       "1  is upset that he can't update his Facebook by ...          111          21   \n",
       "2  @Kenichan I dived many times for the ball. Man...           89          18   \n",
       "3    my whole body feels itchy and like its on fire            47          10   \n",
       "4  @nationwideclass no, it's not behaving at all....          111          21   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  hour  day_of_week  month  \n",
       "0              0              1          1    22            0      4  \n",
       "1              0              0          0    22            0      4  \n",
       "2              0              1          0    22            0      4  \n",
       "3              0              0          0    22            0      4  \n",
       "4              0              1          0    22            0      4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "df_all = load_and_preprocess_data('sentiment140.csv')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "Dataset shape: (50000, 14)\n",
      "Sentiment distribution: sentiment\n",
      "1    0.50028\n",
      "0    0.49972\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Extracting Word2Vec features...\n",
      "\n",
      "Preparing features with selective PCA...\n",
      "Numerical PCA: 4 components explain 70.51% of variance\n",
      "\n",
      "Top 4 features for each of the first 4 principal components:\n",
      "\n",
      "Component 1 (explains 26.75% of variance):\n",
      "  + user_sentiment_std: 0.5918\n",
      "  + user_tweet_count: 0.5613\n",
      "  + avg_posting_gap_seconds: 0.5430\n",
      "  + mention_count: 0.1921\n",
      "\n",
      "Component 2 (explains 15.47% of variance):\n",
      "  + word_count: 0.6287\n",
      "  + mention_count: 0.5913\n",
      "  - url_count: 0.4509\n",
      "  + hashtag_count: 0.1565\n",
      "\n",
      "Component 3 (explains 14.69% of variance):\n",
      "  + hashtag_count: 0.7655\n",
      "  + url_count: 0.5845\n",
      "  + word_count: 0.2657\n",
      "  - mention_count: 0.0410\n",
      "\n",
      "Component 4 (explains 13.61% of variance):\n",
      "  + hashtag_count: 0.6222\n",
      "  - url_count: 0.5818\n",
      "  - word_count: 0.5201\n",
      "  - mention_count: 0.0503\n",
      "\n",
      "Cumulative variance explained by these 4 components: 70.51%\n"
     ]
    }
   ],
   "source": [
    "df = df_all.sample(50000, replace=False, random_state=42)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution: {df['sentiment'].value_counts(normalize=True)}\")\n",
    "\n",
    "# After feature engineering\n",
    "df = engineer_features(df)\n",
    "\n",
    "# Extract Word2Vec features normally (without PCA)\n",
    "print(\"\\nExtracting Word2Vec features...\")\n",
    "w2v_df, w2v_model = extract_word2vec_features(df, vector_size=25)  # Use smaller vector size\n",
    "\n",
    "# Apply selective PCA approach\n",
    "print(\"\\nPreparing features with selective PCA...\")\n",
    "features_df, numerical_pca, categorical_encoder = prepare_features_with_selective_pca(\n",
    "    df, w2v_df, num_pca_components=4\n",
    ")\n",
    "\n",
    "# Define feature groups\n",
    "numerical_features = ['word_count', 'hashtag_count', \n",
    "                        'mention_count', 'url_count', 'user_tweet_count', \n",
    "                        'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "\n",
    "# Analyze the PCA components\n",
    "analyze_pca_components(\n",
    "    pca=numerical_pca,\n",
    "    feature_names=df[numerical_features].columns,  # Original feature names\n",
    "    n_components=4,  # Show top 4 components\n",
    "    n_top_features=4  # Show top 4 features per component\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Training SVM took 127.90 seconds\n",
      "SVM Accuracy: 0.7451\n",
      "Confusion Matrix:\n",
      "[[3715 1282]\n",
      " [1267 3736]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74      4997\n",
      "           1       0.74      0.75      0.75      5003\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Training Random Forest took 39.03 seconds\n",
      "Random Forest Accuracy: 0.7306\n",
      "Confusion Matrix:\n",
      "[[3623 1374]\n",
      " [1320 3683]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      4997\n",
      "           1       0.73      0.74      0.73      5003\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.73      0.73      0.73     10000\n",
      "weighted avg       0.73      0.73      0.73     10000\n",
      "\n",
      "==================================================\n",
      "Training XGBoost...\n",
      "Training XGBoost took 122.67 seconds\n",
      "XGBoost Accuracy: 0.7363\n",
      "Confusion Matrix:\n",
      "[[3677 1320]\n",
      " [1317 3686]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74      4997\n",
      "           1       0.74      0.74      0.74      5003\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.74      0.74      0.74     10000\n",
      "weighted avg       0.74      0.74      0.74     10000\n",
      "\n",
      "==================================================\n",
      "Training Ensemble (Majority Voting)...\n",
      "Training Ensemble took 288.88 seconds\n",
      "Ensemble Accuracy: 0.7413\n",
      "Confusion Matrix:\n",
      "[[3696 1301]\n",
      " [1286 3717]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74      4997\n",
      "           1       0.74      0.74      0.74      5003\n",
      "\n",
      "    accuracy                           0.74     10000\n",
      "   macro avg       0.74      0.74      0.74     10000\n",
      "weighted avg       0.74      0.74      0.74     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    features_df, df['sentiment'], df.index, test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    ")\n",
    "\n",
    "# Train models\n",
    "results = train_evaluate_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best Random Forest parameters: {'max_depth': 20, 'n_estimators': 200}\n",
      "Best XGBoost parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # SVM - Focus on kernel, C, and gamma (most impactful for SVM)\n",
    "# svm_param_grid = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'kernel': ['linear', 'rbf'],  # Removed 'poly' to simplify\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# svm_grid = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# svm_grid.fit(X_train, y_train)\n",
    "# print(f\"Best SVM parameters: {svm_grid.best_params_}\")\n",
    "# best_svm = svm_grid.best_estimator_\n",
    "\n",
    "# # Random Forest - Focus on n_estimators and max_depth\n",
    "# rf_param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [None, 20]\n",
    "# }\n",
    "\n",
    "# rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# rf_grid.fit(X_train, y_train)\n",
    "# print(f\"Best Random Forest parameters: {rf_grid.best_params_}\")\n",
    "# best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# # XGBoost - Focus on n_estimators, learning_rate and max_depth\n",
    "# xgb_param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "# xgb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# xgb_grid.fit(X_train, y_train)\n",
    "# print(f\"Best XGBoost parameters: {xgb_grid.best_params_}\")\n",
    "# best_xgb = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "Performing Time-based Cross Validation\n",
      "Time-based CV took 809.85 seconds\n",
      "Time-based CV Scores: [0.7395895835833434, 0.7353894155766231, 0.7389895595823833, 0.7451098043921757, 0.750030001200048]\n",
      "Mean Time-based CV Score: 0.7418\n",
      "\n",
      "Performing User-based Cross Validation\n",
      "User-based CV took 1443.48 seconds\n",
      "User-based CV Scores: [0.7465, 0.7482, 0.7482, 0.7419, 0.7423]\n",
      "Mean User-based CV Score: 0.7454\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming cross-validation...\")\n",
    "y = df['sentiment']\n",
    "time_scores, user_scores = perform_cross_validation(features_df, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassified examples...\n",
      "Number of misclassified examples: 2549\n",
      "\n",
      "Misclassification Analysis by Features:\n",
      "\n",
      "By Text Length:\n",
      "text_length_bin\n",
      "Very Short    0.302079\n",
      "Short         0.407611\n",
      "Medium        0.289525\n",
      "Long          0.000785\n",
      "Very Long     0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By User Tweet Count:\n",
      "user_tweet_count_bin\n",
      "Very Few     0.988231\n",
      "Few          0.010985\n",
      "Average      0.000785\n",
      "Many         0.000000\n",
      "Very Many    0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By Hour of Day:\n",
      "hour_bin\n",
      "Night        0.314179\n",
      "Morning      0.274080\n",
      "Afternoon    0.199669\n",
      "Evening      0.212071\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample of Misclassified Examples:\n",
      "Text: @jkldesign I promise to keep you pepped up if you do the same for me...as I have a million things to do also lol. Sorry about the tubes \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: @MissKeriBaby couldn't make it 2 the show  afterparty???\n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: @cocoteotico Why would you want to get 100 followers? ) ) I miss you.  You dont reply anymore...\n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: @jodi_lyn So glad you guys liked ST! No news from my end on R2 \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: The light is off and on and off and on and off again now \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing misclassified examples...\")\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "misclassified_df = analyze_misclassified_examples(df, X_test, y_test, best_model, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating visualizations...\n",
      "\n",
      "Comparing processing methods...\n",
      "Testing Local Python Implementation...\n",
      "Local processing time: 0.09 seconds\n",
      "\n",
      "Testing Parallel Processing Implementation...\n",
      "Using 8 cores\n",
      "Error in parallel processing: Can't pickle local object 'compare_processing_methods.<locals>.process_chunk'\n",
      "Please set up a proper distributed environment for actual testing\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "visualize_results(df, results, numerical_pca, numerical_features)\n",
    "\n",
    "print(\"\\nComparing processing methods...\")\n",
    "performance_results = compare_processing_methods(df)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
