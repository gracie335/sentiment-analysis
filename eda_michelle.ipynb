{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blThwcZo0Q-V"
   },
   "source": [
    "### Research questions\n",
    "1. prediction problem (classification) and feature weights - simply predict the next sentiment based on generated features, and analyse which feature contributes the most. examples: user-id (same thing) date (time), text (key words).\n",
    "2. incoperate with LLM to give explanations of why the text is classified as given sentiment.\n",
    "3. efficient forecasting over large datasets, create a basic model, and compared two ways of processing data. 1, deploy locally and use naive python packages. 2, utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1744828695063,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "gCxBRGXvwynN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GroupKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "error",
     "timestamp": 1744828701560,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "4XMFiO5Lw3RQ",
    "outputId": "0fd86710-1545-44ab-fb06-88c2fcbf0ed6"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path, encoding='latin-1', header=None)\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "    \n",
    "    # Convert sentiment to binary (0: negative, 1: positive)\n",
    "    # Assuming sentiment values are 0 and 4 in the original dataset\n",
    "    df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S PDT %Y')\n",
    "    \n",
    "    # Extract basic features from text\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['hashtag_count'] = df['text'].str.count(r'#')\n",
    "    df['mention_count'] = df['text'].str.count(r'@')\n",
    "    df['url_count'] = df['text'].str.count(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text data by removing @mentions, URLs, hashtags, punctuation\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs - more comprehensive pattern\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions - more comprehensive pattern\n",
    "    text = re.sub(r'@[\\w_]+', '', text)\n",
    "    \n",
    "    # Remove hashtags (but keep the text after #)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove digits (optional - uncomment if needed)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace (including newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Function to debug text cleaning process\n",
    "    \"\"\"\n",
    "    print(\"Original:\", text)\n",
    "    \n",
    "    # Test URL removal\n",
    "    text_no_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    print(\"After URL removal:\", text_no_urls)\n",
    "    \n",
    "    # Test @mention removal\n",
    "    text_no_mentions = re.sub(r'@[\\w_]+', '', text_no_urls)\n",
    "    print(\"After @mention removal:\", text_no_mentions)\n",
    "    \n",
    "    # Test hashtag conversion\n",
    "    text_no_hashtags = re.sub(r'#(\\w+)', r'\\1', text_no_mentions)\n",
    "    print(\"After hashtag conversion:\", text_no_hashtags)\n",
    "    \n",
    "    # Test punctuation removal\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', '', text_no_hashtags)\n",
    "    print(\"After punctuation removal:\", text_no_punct)\n",
    "    \n",
    "    # Test whitespace cleaning\n",
    "    text_clean = re.sub(r'\\s+', ' ', text_no_punct).strip()\n",
    "    print(\"Final cleaned:\", text_clean)\n",
    "    \n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h0QoPhx7zY4r"
   },
   "outputs": [],
   "source": [
    "def create_visualizations(df):\n",
    "    # 1. Sentiment Distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='sentiment', data=df)\n",
    "    plt.title('Distribution of Sentiments')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Text Length Distribution by Sentiment\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='sentiment', y='text_length', data=df)\n",
    "    plt.title('Text Length Distribution by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Text Length')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/text_length_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Time-based Analysis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Hourly distribution\n",
    "    sns.countplot(x='hour', hue='sentiment', data=df, ax=axes[0])\n",
    "    axes[0].set_title('Tweets by Hour of Day')\n",
    "    axes[0].set_xlabel('Hour')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Day of week distribution\n",
    "    sns.countplot(x='day_of_week', hue='sentiment', data=df, ax=axes[1])\n",
    "    axes[1].set_title('Tweets by Day of Week')\n",
    "    axes[1].set_xlabel('Day of Week (0=Monday)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Monthly distribution\n",
    "    sns.countplot(x='month', hue='sentiment', data=df, ax=axes[2])\n",
    "    axes[2].set_title('Tweets by Month')\n",
    "    axes[2].set_xlabel('Month')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/time_based_analysis.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Feature Correlation Analysis\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[['sentiment', 'text_length', 'word_count', 'hashtag_count', 'mention_count', 'url_count']].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.savefig('fig/feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Word Clouds for Positive and Negative Tweets\n",
    "    def generate_wordcloud(text, title, filename):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    \n",
    "    # Generate word clouds for positive and negative tweets\n",
    "    positive_text = ' '.join(df[df['sentiment'] == 1]['text'])\n",
    "    negative_text = ' '.join(df[df['sentiment'] == 0]['text'])\n",
    "    \n",
    "    generate_wordcloud(positive_text, 'Word Cloud for Positive Tweets', 'positive_wordcloud.png')\n",
    "    generate_wordcloud(negative_text, 'Word Cloud for Negative Tweets', 'negative_wordcloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features based on the project outline\n",
    "    \"\"\"\n",
    "    # 1. User-based Features\n",
    "    \n",
    "    # Group by user and calculate statistics\n",
    "    user_stats = df.groupby('user')['sentiment'].agg(['mean', 'count', 'std']).reset_index()\n",
    "    \n",
    "    # Calculate correct std with n-1 denominator\n",
    "    def adjusted_std(group):\n",
    "        if len(group) <= 1:\n",
    "            return 0\n",
    "        return np.std(group, ddof=1)  # ddof=1 uses n-1 denominator\n",
    "    \n",
    "    user_sentiment_std = df.groupby('user')['sentiment'].apply(adjusted_std)\n",
    "    user_stats['std'] = user_stats['user'].map(user_sentiment_std)\n",
    "    \n",
    "    # Handle case where a user has only one tweet (std is NaN)\n",
    "    user_stats['std'] = user_stats['std'].fillna(0)\n",
    "    \n",
    "    user_stats.columns = ['user', 'user_avg_sentiment', 'user_tweet_count', 'user_sentiment_std']\n",
    "    \n",
    "    # Merge user stats back to main dataframe\n",
    "    df = pd.merge(df, user_stats, on='user', how='left')\n",
    "    \n",
    "    # Calculate average posting gap time for each user\n",
    "    df = df.sort_values(['user', 'date'])\n",
    "    \n",
    "    # Function to calculate average time between posts\n",
    "    def calc_avg_gap(group):\n",
    "        if len(group) <= 1:\n",
    "            return pd.Timedelta(0)\n",
    "        gaps = group['date'].diff().dropna()\n",
    "        return gaps.mean()\n",
    "    \n",
    "    # Calculate average gap for each user\n",
    "    avg_gaps = df.groupby('user').apply(calc_avg_gap)\n",
    "    avg_gaps_seconds = avg_gaps.dt.total_seconds()\n",
    "    avg_gaps_df = pd.DataFrame({\n",
    "        'user': avg_gaps.index, \n",
    "        'avg_posting_gap_seconds': avg_gaps_seconds.values\n",
    "    })\n",
    "    \n",
    "    # Merge gaps back to main dataframe\n",
    "    df = pd.merge(df, avg_gaps_df, on='user', how='left')\n",
    "    df['avg_posting_gap_seconds'] = df['avg_posting_gap_seconds'].fillna(0)\n",
    "    \n",
    "    # 2. Text Processing Features\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Create tokenized text for Word2Vec\n",
    "    df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    # These will be one-hot encoded later\n",
    "    df['hour_cat'] = df['hour'].astype('category')\n",
    "    df['day_of_week_cat'] = df['day_of_week'].astype('category') \n",
    "    df['month_cat'] = df['month'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_features(df, w2v_df):\n",
    "    \"\"\"\n",
    "    Prepare features with proper preprocessing\n",
    "    \"\"\"\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = ['hour_cat', 'day_of_week_cat', 'month_cat']\n",
    "    numerical_features = ['text_length', 'word_count', 'hashtag_count', \n",
    "                         'mention_count', 'url_count', 'user_tweet_count', \n",
    "                         'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "    \n",
    "    # Create column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Extract features from DataFrame\n",
    "    X = df[numerical_features + categorical_features]\n",
    "    \n",
    "    # Preprocess the features\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Combine with Word2Vec features (which may need their own scaling)\n",
    "    w2v_scaled = StandardScaler().fit_transform(w2v_df)\n",
    "    \n",
    "    # Convert to sparse matrices if needed for efficiency\n",
    "    from scipy import sparse\n",
    "    if sparse.issparse(X_preprocessed):\n",
    "        X_combined = sparse.hstack([X_preprocessed, w2v_scaled])\n",
    "    else:\n",
    "        X_combined = np.hstack([X_preprocessed, w2v_scaled])\n",
    "    \n",
    "    return X_combined, preprocessor\n",
    "\n",
    "\n",
    "def extract_word2vec_features(df, vector_size=25, min_count=2):\n",
    "    \"\"\"\n",
    "    Extract Word2Vec features\n",
    "    \"\"\"\n",
    "    # Train Word2Vec model\n",
    "    all_tokens = df['tokens'].tolist()\n",
    "\n",
    "      # Train with more context window and more training iterations\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=all_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=8,          # Larger context window\n",
    "        min_count=min_count, # Ignore rare words\n",
    "        workers=4,\n",
    "        sg=1,              # Use skip-gram\n",
    "        epochs=20          # More training iterations\n",
    "    )\n",
    "    \n",
    "    # Function to get document vectors by averaging word vectors\n",
    "    def get_doc_vector(tokens):\n",
    "        vec = np.zeros(vector_size)\n",
    "        count = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                vec += w2v_model.wv[word]\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                # Word not in vocabulary\n",
    "                continue\n",
    "        if count > 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    \n",
    "    # Get document vectors\n",
    "    \n",
    "    doc_vectors = np.array(df['tokens'].apply(get_doc_vector).tolist())\n",
    "    w2v_df = pd.DataFrame(\n",
    "        doc_vectors,\n",
    "        columns=[f'w2v_{i}' for i in range(vector_size)]\n",
    "    )\n",
    "    # doc_vectors = df['tokens'].apply(get_doc_vector)\n",
    "    # w2v_df = pd.DataFrame(doc_vectors, columns=['word2vec_embedding'])\n",
    "\n",
    "    return w2v_df, w2v_model\n",
    "\n",
    "def select_features(features_df, n_components=20):\n",
    "    \"\"\"\n",
    "    Perform PCA for feature selection\n",
    "    \"\"\"\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    pca_features = pca.fit_transform(features_df)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_features, \n",
    "        columns=[f'pca_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Print variance explanation\n",
    "    print(f\"Top 10 components explain {cumulative_variance[9]:.2%} of variance\")\n",
    "    print(f\"All {n_components} components explain {cumulative_variance[-1]:.2%} of variance\")\n",
    "    \n",
    "    return pca_df, pca\n",
    "\n",
    "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate classification models\n",
    "    \"\"\"\n",
    "    # Define base models\n",
    "    svm = SVC(probability=False, kernel='rbf', random_state=42)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    xgb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Train individual models\n",
    "    models = {\n",
    "        'SVM': svm,\n",
    "        'Random Forest': rf,\n",
    "        'XGBoost': xgb\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report\n",
    "        }\n",
    "        \n",
    "        print(f\"Training {name} took {time.time() - t0:.2f} seconds\")\n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "        print(f\"Classification Report:\\n{class_report}\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # Create Voting Ensemble (majority voting)\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "        voting='hard'  # Majority voting\n",
    "    )\n",
    "    \n",
    "    print(\"Training Ensemble (Majority Voting)...\")\n",
    "    t0 = time.time()\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results['Ensemble'] = {\n",
    "        'model': voting_clf,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "    print(\"Training Ensemble took %0.2f seconds\" % (time.time() - t0))\n",
    "    print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_cross_validation(X, y, df):\n",
    "    \"\"\"\n",
    "    Perform time-based and user-based cross-validation\n",
    "    \"\"\"\n",
    "    # Time-based Cross Validation\n",
    "    print(\"Performing Time-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Define base models, use voting ensemble for cross validation\n",
    "    svm = SVC(probability=False, kernel='rbf', random_state=42)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    xgb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    time_scores = []\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Create Voting Ensemble (majority voting)\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "            voting='hard'  # Majority voting\n",
    "        )\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        time_scores.append(score)\n",
    "    \n",
    "    print(f\"Time-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"Time-based CV Scores: {time_scores}\")\n",
    "    print(f\"Mean Time-based CV Score: {np.mean(time_scores):.4f}\")\n",
    "    \n",
    "    # User-based Cross Validation\n",
    "    print(\"\\nPerforming User-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    user_groups = df['user'].astype('category').cat.codes.values\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    user_scores = []\n",
    "    for train_index, test_index in gkf.split(X, y, groups=user_groups):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Create Voting Ensemble (majority voting)\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "            voting='hard'  # Majority voting\n",
    "        )\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        user_scores.append(score)\n",
    "    \n",
    "    print(f\"User-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"User-based CV Scores: {user_scores}\")\n",
    "    print(f\"Mean User-based CV Score: {np.mean(user_scores):.4f}\")\n",
    "    \n",
    "    return time_scores, user_scores\n",
    "\n",
    "def analyze_misclassified_examples(df, X_test, y_test, model, idx_test):\n",
    "    \"\"\"\n",
    "    Analyze misclassified examples\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    misclassified_idx = idx_test[y_pred != y_test]\n",
    "    \n",
    "    misclassified_df = df.iloc[misclassified_idx].copy()\n",
    "    misclassified_df['predicted_sentiment'] = y_pred[y_pred != y_test]\n",
    "    \n",
    "    print(f\"Number of misclassified examples: {len(misclassified_df)}\")\n",
    "    \n",
    "    # Analyze by features\n",
    "    print(\"\\nMisclassification Analysis by Features:\")\n",
    "    \n",
    "    # By text length\n",
    "    print(\"\\nBy Text Length:\")\n",
    "    bins = [0, 50, 100, 150, 200, np.inf]\n",
    "    labels = ['Very Short', 'Short', 'Medium', 'Long', 'Very Long']\n",
    "    misclassified_df['text_length_bin'] = pd.cut(misclassified_df['text_length'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['text_length_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By user tweet count\n",
    "    print(\"\\nBy User Tweet Count:\")\n",
    "    bins = [0, 5, 10, 20, 50, np.inf]\n",
    "    labels = ['Very Few', 'Few', 'Average', 'Many', 'Very Many']\n",
    "    misclassified_df['user_tweet_count_bin'] = pd.cut(misclassified_df['user_tweet_count'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['user_tweet_count_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By time of day\n",
    "    print(\"\\nBy Hour of Day:\")\n",
    "    hour_bins = [0, 6, 12, 18, 24]\n",
    "    hour_labels = ['Night', 'Morning', 'Afternoon', 'Evening']\n",
    "    misclassified_df['hour_bin'] = pd.cut(misclassified_df['hour'], bins=hour_bins, labels=hour_labels)\n",
    "    print(misclassified_df['hour_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Sample of misclassified examples\n",
    "    print(\"\\nSample of Misclassified Examples:\")\n",
    "    sample = misclassified_df.sample(min(5, len(misclassified_df)))\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"Text: {row['text']}\")\n",
    "        print(f\"True Sentiment: {row['sentiment']}, Predicted: {row['predicted_sentiment']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return misclassified_df\n",
    "\n",
    "def visualize_results(df, results, pca, feature_names):\n",
    "    \"\"\"\n",
    "    Create visualizations for the analysis\n",
    "    \"\"\"\n",
    "    # 1. PCA Explained Variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Principal Component')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/pca_variance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Feature Importance from PCA loadings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Get most important features from first component\n",
    "    component = 0\n",
    "    loadings = pd.Series(abs(pca.components_[component]), index=feature_names)\n",
    "    top_features = loadings.nlargest(15)\n",
    "    \n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.title(f'Top 15 Feature Importances (PC {component+1})')\n",
    "    plt.xlabel('Absolute Loading Value')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Sentiment Distribution by Time of Day\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hour_counts = df.groupby(['hour', 'sentiment']).size().unstack()\n",
    "    hour_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_hour.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Sentiment Distribution by Day of Week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_counts = df.groupby(['day_of_week', 'sentiment']).size().unstack()\n",
    "    day_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Day of Week')\n",
    "    plt.xlabel('Day of Week (0=Monday)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_day.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. User Sentiment Patterns (Top 10 users by tweet count)\n",
    "    top_users = df['user'].value_counts().head(10).index\n",
    "    user_df = df[df['user'].isin(top_users)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    user_sentiment = user_df.groupby('user')['sentiment'].mean().sort_values()\n",
    "    sns.barplot(x=user_sentiment.index, y=user_sentiment.values)\n",
    "    plt.title('Average Sentiment for Top 10 Users')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/user_sentiment.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Word Clouds by Sentiment\n",
    "    for sentiment, label in [(0, 'Negative'), (1, 'Positive')]:\n",
    "        text = ' '.join(df[df['sentiment'] == sentiment]['clean_text'])\n",
    "        \n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            max_words=200\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for {label} Sentiment')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/wordcloud_sentiment_{sentiment}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Model Comparison\n",
    "    accuracies = {name: info['accuracy'] for name, info in results.items()}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Confusion Matrix Visualization\n",
    "    for name, info in results.items():\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = info['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/confusion_matrix_{name}.png')\n",
    "        plt.close()\n",
    "\n",
    "def compare_processing_methods(df, test_size=1000):\n",
    "    \"\"\"\n",
    "    Compare local vs distributed processing performance\n",
    "    \"\"\"\n",
    "    # Subset data for testing\n",
    "    test_df = df.sample(test_size, random_state=42)\n",
    "    \n",
    "    # 1. Local Python Implementation\n",
    "    print(\"Testing Local Python Implementation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate local processing\n",
    "    tokens = test_df['text'].apply(clean_text).apply(word_tokenize).tolist()\n",
    "    local_time = time.time() - start_time\n",
    "    print(f\"Local processing time: {local_time:.2f} seconds\")\n",
    "    \n",
    "    # 2. Simulated Distributed Processing\n",
    "    try:\n",
    "        print(\"\\nTesting Parallel Processing Implementation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine number of cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(f\"Using {num_cores} cores\")\n",
    "        \n",
    "        # Split data into chunks\n",
    "        chunks = np.array_split(test_df['text'], num_cores)\n",
    "        \n",
    "        # Define processing function\n",
    "        def process_chunk(chunk):\n",
    "            return [word_tokenize(clean_text(text)) for text in chunk]\n",
    "        \n",
    "        # Create a pool and process in parallel\n",
    "        with multiprocessing.Pool(num_cores) as pool:\n",
    "            results = pool.map(process_chunk, chunks)\n",
    "            \n",
    "        # Flatten results\n",
    "        parallel_tokens = [item for sublist in results for item in sublist]\n",
    "        \n",
    "        parallel_time = time.time() - start_time\n",
    "        print(f\"Parallel processing time: {parallel_time:.2f} seconds\")\n",
    "        print(f\"Speedup: {local_time / parallel_time:.2f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in parallel processing: {e}\")\n",
    "        print(\"Please set up a proper distributed environment for actual testing\")\n",
    "        parallel_time = None\n",
    "    \n",
    "    return {'local_time': local_time, 'parallel_time': parallel_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_with_selective_pca(df, w2v_df, num_pca_components=10):\n",
    "    \"\"\"\n",
    "    Apply PCA selectively to numerical features only, then combine with\n",
    "    categorical features and Word2Vec embeddings\n",
    "    \"\"\"\n",
    "    # Define feature groups\n",
    "    numerical_features = ['word_count', 'hashtag_count', \n",
    "                         'mention_count', 'url_count', 'user_tweet_count', \n",
    "                         'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "    \n",
    "    categorical_features = ['hour', 'day_of_week', 'month']\n",
    "    \n",
    "    # 1. Extract and standardize numerical features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    numerical_scaled = scaler.fit_transform(df[numerical_features])\n",
    "    \n",
    "    # 2. Apply PCA to numerical features only\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=min(num_pca_components, len(numerical_features)))\n",
    "    numerical_pca = pca.fit_transform(numerical_scaled)\n",
    "    \n",
    "    # Print variance explained by numerical PCA\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    print(f\"Numerical PCA: {pca.n_components_} components explain {cumulative_variance[-1]:.2%} of variance\")\n",
    "    \n",
    "    # 3. One-hot encode categorical features\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
    "    \n",
    "    # 4. Combine all features: numerical_pca + categorical_encoded + w2v_df\n",
    "    \n",
    "    # Convert to numeric arrays if needed\n",
    "    numerical_pca_array = numerical_pca\n",
    "    w2v_array = w2v_df.values\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_features = np.hstack([\n",
    "        numerical_pca_array,   # PCA-reduced numerical features\n",
    "        categorical_encoded,   # One-hot encoded categorical features\n",
    "        w2v_array              # Word2Vec embeddings\n",
    "    ])\n",
    "    \n",
    "    # Create feature names for interpretability\n",
    "    feature_names = (\n",
    "        [f'num_pca_{i}' for i in range(pca.n_components_)] +\n",
    "        [f'{feat}_{cat}' for feat, cats in zip(encoder.feature_names_in_, \n",
    "                                              encoder.categories_) \n",
    "                          for cat in cats[1:]] +\n",
    "        list(w2v_df.columns)\n",
    "    )\n",
    "    \n",
    "    # Return as DataFrame for convenience\n",
    "    combined_df = pd.DataFrame(combined_features, columns=feature_names)\n",
    "    \n",
    "    return combined_df, pca, encoder\n",
    "\n",
    "\n",
    "def analyze_pca_components(pca, feature_names, n_components=4, n_top_features=4):\n",
    "    \"\"\"\n",
    "    Analyze PCA components and print the top features for each component\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca : PCA\n",
    "        Fitted PCA model\n",
    "    feature_names : list\n",
    "        Names of the features used in PCA\n",
    "    n_components : int\n",
    "        Number of principal components to analyze\n",
    "    n_top_features : int\n",
    "        Number of top features to display for each component\n",
    "    \"\"\"\n",
    "    # Check if we have fewer components than requested\n",
    "    n_components = min(n_components, pca.n_components_)\n",
    "    \n",
    "    print(f\"\\nTop {n_top_features} features for each of the first {n_components} principal components:\")\n",
    "    \n",
    "    # For each component\n",
    "    for i in range(n_components):\n",
    "        # Get loadings (weights) for this component\n",
    "        loadings = pca.components_[i]\n",
    "        \n",
    "        # Get indices of top features (highest absolute loadings)\n",
    "        top_indices = np.argsort(np.abs(loadings))[-n_top_features:]\n",
    "        \n",
    "        # Reverse to get highest first\n",
    "        top_indices = top_indices[::-1]\n",
    "        \n",
    "        # Print component number and variance explained\n",
    "        variance = pca.explained_variance_ratio_[i]\n",
    "        print(f\"\\nComponent {i+1} (explains {variance:.2%} of variance):\")\n",
    "        \n",
    "        # Print top features\n",
    "        for idx in top_indices:\n",
    "            # Get feature name and loading\n",
    "            feature = feature_names[idx]\n",
    "            loading = loadings[idx]\n",
    "            sign = \"+\" if loading > 0 else \"-\"\n",
    "            \n",
    "            # Print feature and its loading\n",
    "            print(f\"  {sign} {feature}: {abs(loading):.4f}\")\n",
    "    \n",
    "    # Print cumulative variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    print(f\"\\nCumulative variance explained by these {n_components} components: {cumulative_variance[n_components-1]:.2%}\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                date     query             user  \\\n",
       "0          0  1467810369 2009-04-06 22:19:45  NO_QUERY  _TheSpecialOne_   \n",
       "1          0  1467810672 2009-04-06 22:19:49  NO_QUERY    scotthamilton   \n",
       "2          0  1467810917 2009-04-06 22:19:53  NO_QUERY         mattycus   \n",
       "3          0  1467811184 2009-04-06 22:19:57  NO_QUERY          ElleCTF   \n",
       "4          0  1467811193 2009-04-06 22:19:57  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  text_length  word_count  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          115          19   \n",
       "1  is upset that he can't update his Facebook by ...          111          21   \n",
       "2  @Kenichan I dived many times for the ball. Man...           89          18   \n",
       "3    my whole body feels itchy and like its on fire            47          10   \n",
       "4  @nationwideclass no, it's not behaving at all....          111          21   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  hour  day_of_week  month  \n",
       "0              0              1          1    22            0      4  \n",
       "1              0              0          0    22            0      4  \n",
       "2              0              1          0    22            0      4  \n",
       "3              0              0          0    22            0      4  \n",
       "4              0              1          0    22            0      4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "df_all = load_and_preprocess_data('sentiment140.csv')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "Dataset shape: (10000, 14)\n",
      "Sentiment distribution: sentiment\n",
      "0    0.5004\n",
      "1    0.4996\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = df_all.sample(10000, replace=False, random_state=42)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution: {df['sentiment'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Word2Vec features...\n",
      "\n",
      "Preparing features with selective PCA...\n",
      "Numerical PCA: 4 components explain 72.20% of variance\n",
      "\n",
      "Top 4 features for each of the first 4 principal components:\n",
      "\n",
      "Component 1 (explains 27.96% of variance):\n",
      "  + user_tweet_count: 0.5829\n",
      "  + avg_posting_gap_seconds: 0.5712\n",
      "  + user_sentiment_std: 0.5647\n",
      "  + mention_count: 0.1163\n",
      "\n",
      "Component 2 (explains 16.04% of variance):\n",
      "  + word_count: 0.6160\n",
      "  + mention_count: 0.5929\n",
      "  - url_count: 0.4237\n",
      "  + hashtag_count: 0.2809\n",
      "\n",
      "Component 3 (explains 14.48% of variance):\n",
      "  + hashtag_count: 0.7693\n",
      "  + url_count: 0.5284\n",
      "  + word_count: 0.2540\n",
      "  - mention_count: 0.2483\n",
      "\n",
      "Component 4 (explains 13.72% of variance):\n",
      "  - url_count: 0.6925\n",
      "  + hashtag_count: 0.5095\n",
      "  - word_count: 0.4036\n",
      "  - mention_count: 0.3086\n",
      "\n",
      "Cumulative variance explained by these 4 components: 72.20%\n"
     ]
    }
   ],
   "source": [
    "# After feature engineering\n",
    "df = engineer_features(df)\n",
    "\n",
    "# Extract Word2Vec features normally (without PCA)\n",
    "print(\"\\nExtracting Word2Vec features...\")\n",
    "w2v_df, w2v_model = extract_word2vec_features(df, vector_size=25)  # Use smaller vector size\n",
    "\n",
    "# Apply selective PCA approach\n",
    "print(\"\\nPreparing features with selective PCA...\")\n",
    "features_df, numerical_pca, categorical_encoder = prepare_features_with_selective_pca(\n",
    "    df, w2v_df, num_pca_components=4\n",
    ")\n",
    "\n",
    "# Define feature groups\n",
    "numerical_features = ['word_count', 'hashtag_count', \n",
    "                        'mention_count', 'url_count', 'user_tweet_count', \n",
    "                        'user_sentiment_std', 'avg_posting_gap_seconds']\n",
    "\n",
    "# Analyze the PCA components\n",
    "analyze_pca_components(\n",
    "    pca=numerical_pca,\n",
    "    feature_names=df[numerical_features].columns,  # Original feature names\n",
    "    n_components=4,  # Show top 4 components\n",
    "    n_top_features=4  # Show top 4 features per component\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Training SVM took 5.26 seconds\n",
      "SVM Accuracy: 0.7150\n",
      "Confusion Matrix:\n",
      "[[698 303]\n",
      " [267 732]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      1001\n",
      "           1       0.71      0.73      0.72       999\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.72      0.72      0.71      2000\n",
      "weighted avg       0.72      0.71      0.71      2000\n",
      "\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Training Random Forest took 3.24 seconds\n",
      "Random Forest Accuracy: 0.7125\n",
      "Confusion Matrix:\n",
      "[[733 268]\n",
      " [307 692]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72      1001\n",
      "           1       0.72      0.69      0.71       999\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.71      0.71      0.71      2000\n",
      "weighted avg       0.71      0.71      0.71      2000\n",
      "\n",
      "==================================================\n",
      "Training XGBoost...\n",
      "Training XGBoost took 10.40 seconds\n",
      "XGBoost Accuracy: 0.7125\n",
      "Confusion Matrix:\n",
      "[[705 296]\n",
      " [279 720]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      1001\n",
      "           1       0.71      0.72      0.71       999\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.71      0.71      0.71      2000\n",
      "weighted avg       0.71      0.71      0.71      2000\n",
      "\n",
      "==================================================\n",
      "Training Ensemble (Majority Voting)...\n",
      "Training Ensemble took 19.03 seconds\n",
      "Ensemble Accuracy: 0.7215\n",
      "Confusion Matrix:\n",
      "[[717 284]\n",
      " [273 726]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      1001\n",
      "           1       0.72      0.73      0.72       999\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.72      0.72      0.72      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    features_df, df['sentiment'], df.index, test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    ")\n",
    "\n",
    "# Train models\n",
    "results = train_evaluate_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# print(f\"Best parameters: {grid.best_params_}\")\n",
    "# best_svm = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "Performing Time-based Cross Validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerforming cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m time_scores, user_scores \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 265\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[0;34m(X, y, df)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Create Voting Ensemble (majority voting)\u001b[39;00m\n\u001b[1;32m    261\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[1;32m    262\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, svm), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, rf), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb)],\n\u001b[1;32m    263\u001b[0m     voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhard\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Majority voting\u001b[39;00m\n\u001b[1;32m    264\u001b[0m )\n\u001b[0;32m--> 265\u001b[0m \u001b[43mvoting_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    267\u001b[0m score \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    347\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVoting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    607\u001b[0m         )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    250\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    258\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sentiment_analysis/lib/python3.8/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming cross-validation...\")\n",
    "y = df['sentiment']\n",
    "time_scores, user_scores = perform_cross_validation(features_df, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassified examples...\n",
      "Number of misclassified examples: 2994\n",
      "\n",
      "Misclassification Analysis by Features:\n",
      "\n",
      "By Text Length:\n",
      "text_length_bin\n",
      "Very Short    0.301603\n",
      "Short         0.413828\n",
      "Medium        0.283233\n",
      "Long          0.001336\n",
      "Very Long     0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By User Tweet Count:\n",
      "user_tweet_count_bin\n",
      "Very Few     0.987976\n",
      "Few          0.010354\n",
      "Average      0.001670\n",
      "Many         0.000000\n",
      "Very Many    0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By Hour of Day:\n",
      "hour_bin\n",
      "Night        0.310018\n",
      "Morning      0.270650\n",
      "Afternoon    0.209139\n",
      "Evening      0.210193\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample of Misclassified Examples:\n",
      "Text: @dustmine321  My deepest condolences.\n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: mark cubans doin handsprings right now- the &quot;punks and thugs&quot; are sinking.. but he's not looking forward to paying dirk's child support. \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: @Hans_Karl Just a great day with the family....  didn't want it to end!!    And you?\n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: @1outside haha cool. I like this big glass brick one - is it the National Theatre? But I have never been inside \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n",
      "Text: handling the exceptions is boring \n",
      "True Sentiment: 0, Predicted: 1\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing misclassified examples...\")\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "misclassified_df = analyze_misclassified_examples(df, X_test, y_test, best_model, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating visualizations...\n",
      "\n",
      "Comparing processing methods...\n",
      "Testing Local Python Implementation...\n",
      "Local processing time: 0.05 seconds\n",
      "\n",
      "Testing Parallel Processing Implementation...\n",
      "Using 8 cores\n",
      "Error in parallel processing: Can't pickle local object 'compare_processing_methods.<locals>.process_chunk'\n",
      "Please set up a proper distributed environment for actual testing\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "visualize_results(df, results, numerical_pca, numerical_features)\n",
    "\n",
    "print(\"\\nComparing processing methods...\")\n",
    "performance_results = compare_processing_methods(df)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
